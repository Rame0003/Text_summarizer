{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5293 Spring 2020 Project 2\n",
    "## By Abilash Ramesh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from gensim import corpora, models\n",
    "from sklearn.metrics import silhouette_score\n",
    "import networkx\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfiles(path, n):\n",
    "    \n",
    "    filenames=glob.glob(path) ## Get filepaths\n",
    "    filen=len(filenames) \n",
    "    number= random.randint(0,filen) ##Random file index\n",
    "    percent = (n)/100\n",
    "    reqf=(filen) * percent\n",
    "    end=number+reqf\n",
    "    print(\"The files from index %d to %d have been taken\" %(number , end))\n",
    "    print(int(reqf))\n",
    "    taken=filenames[int(number):int(end)]\n",
    "    return taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to normalize text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = text.lower() ## Lowercase text\n",
    "    sents=(nltk.sent_tokenize(text)) ##Sentence tokenization\n",
    "    words=[]\n",
    "    for sent in sents:\n",
    "        sent.strip()\n",
    "        words.extend(nltk.word_tokenize(sent)) ## Word tokenization\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    custom_stop_words = [\n",
    "        'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "        'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "        'al.', 'Elsevier', 'PMC', 'CZI', 'www' \n",
    "    ]\n",
    "    filtered_tokens = [token for token in words if token not in custom_stop_words]\n",
    "    filtered_tokens1 = [token for token in filtered_tokens if token not in custom_stop_words] ##Stop word removal\n",
    "    txt = ' '.join(filtered_tokens1)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to create Dataframe using files list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDB(filepath):\n",
    "    dict_ = {'paper_id': [], 'abstract': [], 'body_text': []} ## Initializing directories\n",
    "    for j in range(len(filepath)):\n",
    "        with open(filepath[j]) as f: ##json text extraction\n",
    "                data=json.load(f)\n",
    "                paper_id = data['paper_id']\n",
    "                abstract = []\n",
    "                body_text = []\n",
    "                for entry in data['abstract']:\n",
    "                    abstract.append(entry['text'])             \n",
    "                for entry in data['body_text']:\n",
    "                    body_text.append(entry['text'])\n",
    "                    \n",
    "                abstract = '\\n'.join(abstract)\n",
    "                body_text = '\\n'.join(body_text)\n",
    "\n",
    "                dict_['paper_id'].append(paper_id)\n",
    "                if len(abstract) == 0: \n",
    "            # if no abstract is provided\n",
    "                    dict_['abstract'].append(\"Not provided.\") ##\n",
    "                else:\n",
    "            # abstract is provided\n",
    "                    dict_['abstract'].append(abstract)\n",
    "               # dict_['abstract'].append(abstract)\n",
    "                dict_['body_text'].append(body_text)\n",
    "                \n",
    "                \n",
    "    df = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\n",
    "    df['abstract'] = df['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x)) ## Remove special charecters\n",
    "    df['abstract'] = df['abstract'].apply(lambda x: normalize(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform TextRank summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansed( Unfinished, txt, n):\n",
    "    sent_tokens=nltk.sent_tokenize(txt)\n",
    "    unfin = nltk.sent_tokenize(Unfinished)\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=2**12, smooth_idf=True, use_idf=True, ngram_range=(2,4))\n",
    "    docu=vectorizer.fit_transform(sent_tokens)\n",
    "    sim_mat= docu*docu.T\n",
    "    sim_graph= networkx.from_scipy_sparse_matrix(sim_mat)\n",
    "    scores = networkx.pagerank(sim_graph)\n",
    "    ranked_sentences = sorted(((score, index)\n",
    "                            for index, score in scores.items()), reverse=True)\n",
    "    top_sentence_indices = [ranked_sentences[index][1] for index in range(0,n)]\n",
    "    top_sentence_indices.sort()\n",
    "    top_sentences = [unfin[index] for index in top_sentence_indices]\n",
    "    summary =''.join(top_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to write summary output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputfiles(dataframe):\n",
    "    for i in range(len(dataframe)):\n",
    "        j=i+1\n",
    "        filename = ('output_%d.md'%(j))\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('This is the output for cluster #%d\\n\\n'%(j))\n",
    "            for text in dataframe['summary'][i]:\n",
    "                f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files from index 187 to 248 have been taken\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "filenames = readfiles('json files/*.json', 20)\n",
    "df = createDB(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(stop_words='english', max_features=2**12, smooth_idf=True, use_idf=True, ngram_range=(2,4))\n",
    "docu=vectorizer.fit_transform(df['abstract'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering using KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.522680508593631\n"
     ]
    }
   ],
   "source": [
    "cluster=np.sqrt(int(len(filenames))/2)\n",
    "print(cluster)\n",
    "kmeans = MiniBatchKMeans(n_clusters=int(cluster),max_iter=5000, init='random')\n",
    "preds = kmeans.fit_predict(docu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe with clustered text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster']=preds\n",
    "df1 = df.groupby('cluster')['body_text'].apply(list).reset_index(name='text')\n",
    "df1['text'] = df1['text'].apply(lambda x:  ' '.join(map(str, x)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "df1['Normalized_text'] = 0\n",
    "\n",
    "for j in range(len(df1)):\n",
    "    df1['Normalized_text'][j]=normalize(df1['text'][j])\n",
    "    \n",
    "df1['summary'] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(df1)):\n",
    "    print(k)\n",
    "    df1['summary'][k]= cleansed(df1['text'][k], df1['Normalized_text'][k], 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfiles(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

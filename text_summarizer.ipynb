{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from gensim import corpora, models\n",
    "from sklearn.metrics import silhouette_score\n",
    "import networkx\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer \n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfiles(path, n):\n",
    "    filenames=glob.glob(path)\n",
    "    filen=len(filenames)\n",
    "    number= random.randint(0,filen)\n",
    "    percent = (n)/100\n",
    "    reqf=(filen)* percent\n",
    "    end=number+reqf\n",
    "    print(\"The files from index %d to %d have been taken\" %(number , end))\n",
    "    print(int(reqf))\n",
    "    taken=filenames[int(number):int(end)]\n",
    "    return taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDB(filepath):\n",
    "    dict_ = {'paper_id': [], 'abstract': [], 'body_text': []}\n",
    "    for j in range(len(filepath)):\n",
    "        with open(filepath[j]) as f:\n",
    "                data=json.load(f)\n",
    "                paper_id = data['paper_id']\n",
    "                abstract = []\n",
    "                body_text = []\n",
    "                for entry in data['abstract']:\n",
    "                    abstract.append(entry['text'])             \n",
    "                for entry in data['body_text']:\n",
    "                    body_text.append(entry['text'])\n",
    "                    \n",
    "                abstract = '\\n'.join(abstract)\n",
    "                body_text = '\\n'.join(body_text)\n",
    "\n",
    "                dict_['paper_id'].append(paper_id)\n",
    "                if len(abstract) == 0: \n",
    "            # no abstract provided\n",
    "                    dict_['abstract'].append(\"Not provided.\")\n",
    "                else:\n",
    "    # abstract is short enough\n",
    "                    dict_['abstract'].append(abstract)\n",
    "               # dict_['abstract'].append(abstract)\n",
    "                dict_['body_text'].append(body_text)\n",
    "                \n",
    "                \n",
    "    df_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text'])\n",
    "    return df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(input_str):\n",
    "    input_str = input_str.lower()\n",
    "    return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    sents=(nltk.sent_tokenize(text))\n",
    "    words=[]\n",
    "    for sent in sents:\n",
    "        sent.strip()\n",
    "        words.extend(nltk.word_tokenize(sent))\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    custom_stop_words = [\n",
    "        'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n",
    "        'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n",
    "        'al.', 'Elsevier', 'PMC', 'CZI', 'www' \n",
    "    ]\n",
    "    filtered_tokens = [token for token in words if token not in custom_stop_words]\n",
    "    filtered_tokens1 = [token for token in filtered_tokens if token not in custom_stop_words]\n",
    "    txt = ' '.join(filtered_tokens1)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleansed(txt, n):\n",
    "    sent_tokens=nltk.sent_tokenize(txt)\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=2**12, smooth_idf=True, use_idf=True, ngram_range=(2,4))\n",
    "    docu=vectorizer.fit_transform(sent_tokens)\n",
    "    sim_mat= docu*docu.T\n",
    "    sim_graph= networkx.from_scipy_sparse_matrix(sim_mat)\n",
    "    scores = networkx.pagerank(sim_graph)\n",
    "    ranked_sentences = sorted(((score, index)\n",
    "                            for index, score in scores.items()), reverse=True)\n",
    "    top_sentence_indices = [ranked_sentences[index][1] for index in range(0,n)]\n",
    "    top_sentence_indices.sort()\n",
    "    top_sentences = [sent_tokens[index] for index in top_sentence_indices]\n",
    "    summary =''.join(top_sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputfiles(dataframe):\n",
    "    for i in range(len(dataframe)):\n",
    "        j=i+1\n",
    "        filename = ('output%d.md'%(j))\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('This is the output for cluster #%d\\n\\n'%(j))\n",
    "            for text in dataframe['summary'][i]:\n",
    "                f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The files from index 21 to 175 have been taken\n",
      "154\n"
     ]
    }
   ],
   "source": [
    "filenames = readfiles('json files/*.json', 50)\n",
    "df = createDB(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract'] = df['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n",
    "df['abstract'] = df['abstract'].apply(lambda x: lower_case(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.774964387392123\n"
     ]
    }
   ],
   "source": [
    "vectorizer=TfidfVectorizer(stop_words='english', max_features=2**12, smooth_idf=True, use_idf=True, ngram_range=(2,4))\n",
    "docu=vectorizer.fit_transform(df['abstract'].values)\n",
    "cluster=np.sqrt(int(len(filenames))/2)\n",
    "print(cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 1, 5, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 3, 5, 5, 5, 7,\n",
       "       5, 5, 5, 1, 1, 5, 1, 5, 5, 1, 5, 2, 1, 5, 7, 5, 5, 5, 5, 5, 1, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 1, 4, 5, 1, 5, 5, 5, 3, 5,\n",
       "       5, 5, 1, 0, 5, 5, 5, 5, 5, 2, 5, 5, 5, 6, 5, 5, 5, 5, 5, 3, 5, 5,\n",
       "       5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 5,\n",
       "       5, 5, 5, 5, 5, 7, 5, 5, 1, 5, 5, 5, 5, 5, 1, 1, 2, 5, 5, 6, 1, 5,\n",
       "       5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 5, 5, 5, 5, 5, 1, 5],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=int(cluster),max_iter=5000, init='random')\n",
    "preds = kmeans.fit_predict(docu)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03252186772996112"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouette_score(docu, kmeans.predict(docu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster']=preds\n",
    "df1 = df.groupby('cluster')['body_text'].apply(list).reset_index(name='text')\n",
    "df1['text'] = df1['text'].apply(lambda x:  ' '.join(map(str, x)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "df1['Normalized_text'] = 0\n",
    "\n",
    "for j in range(len(df1)):\n",
    "    df1['Normalized_text'][j]=normalize(df1['text'][j])\n",
    "    \n",
    "df1['summary'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for k in range(0, len(df1['summary'])):\n",
    "    print(k)\n",
    "    df1['summary'][k]= cleansed(df1['Normalized_text'][k], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>text</th>\n",
       "      <th>Normalized_text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Abstract. We have evaluated the fate of misfol...</td>\n",
       "      <td>Abstract . We have evaluated the fate of misfo...</td>\n",
       "      <td>The hybrid protein carrying the wild-type repr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I nfectious diseases continue to be major heal...</td>\n",
       "      <td>I nfectious diseases continue to be major heal...</td>\n",
       "      <td>Additionally , MERS-CoV has been found to grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Feline infectious peritonitis (FIP) is an immu...</td>\n",
       "      <td>Feline infectious peritonitis ( FIP ) is an im...</td>\n",
       "      <td>Clinical signs associated with CNS disease in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>In December 2019, several patients with pneumo...</td>\n",
       "      <td>In December 2019 , several patients with pneum...</td>\n",
       "      <td>Here , we describe our understanding of the 20...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                               text  \\\n",
       "0        0  Abstract. We have evaluated the fate of misfol...   \n",
       "1        1  I nfectious diseases continue to be major heal...   \n",
       "2        2  Feline infectious peritonitis (FIP) is an immu...   \n",
       "3        3  In December 2019, several patients with pneumo...   \n",
       "\n",
       "                                     Normalized_text  \\\n",
       "0  Abstract . We have evaluated the fate of misfo...   \n",
       "1  I nfectious diseases continue to be major heal...   \n",
       "2  Feline infectious peritonitis ( FIP ) is an im...   \n",
       "3  In December 2019 , several patients with pneum...   \n",
       "\n",
       "                                             summary  \n",
       "0  The hybrid protein carrying the wild-type repr...  \n",
       "1  Additionally , MERS-CoV has been found to grow...  \n",
       "2  Clinical signs associated with CNS disease in ...  \n",
       "3  Here , we describe our understanding of the 20...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputfiles(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
